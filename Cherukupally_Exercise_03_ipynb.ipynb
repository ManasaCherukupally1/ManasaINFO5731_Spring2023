{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManasaCherukupally1/Manasa_INFO5731_Spring2023/blob/main/Cherukupally_Exercise_03_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "## The third In-class-exercise (due on 11:59 PM 10/08/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2htC-oV70ne"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "The interesting text classification task is spam detection in SMS or emails. In this task, the text messages from SMS are emails are analyzed and classified whether it is a spam or\n",
        "not. It is one of the important problem statement as it is very common to get trapped by the spam emails or messages. To keep track of such emails and trying not to become bait to\n",
        "such emails is extremely important. Hence this problem statement is extremely useful Text classification problem statement.\n",
        "\n",
        "The features that can be extracted from this text that will be useful in building the model are\n",
        "1. Word frequency - That count the occurance of each word in the message which will help us in finding the unrelated or rare words like \"congratulations\", \"you have won\" etc\n",
        "2. Existence of hyperlinks(lexical feature extraction) - Usually spam messages contain hyperlinks to to click bait the receivers . Hence it is important to check for hyperlink\n",
        "3. TF/IDF - Term frequency to inverse document frequency is important to know the relevance of word among the total messages in dataset.\n",
        "4. POS tagging - POS tagging is important as the spam messages may contain high number of verbs\n",
        "5. Sentiment score - It is important to know the sentiment score of the message to detect whether it is spam or not\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Download the NLTK stopwords dataset\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Function to clean message\n",
        "def cleaning_text(text):\n",
        "    # Removing punctuation and other special characters\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    # To lowercase\n",
        "    cleaned_text = cleaned_text.lower()\n",
        "    return cleaned_text\n",
        "\n",
        "# Check for hyperlinks\n",
        "def having_hyperlink(text):\n",
        "    return 1 if re.search(r'http[s]?://', text) else 0\n",
        "\n",
        "# Word Frequency features\n",
        "def word_freq_cal(messages):\n",
        "    # Creating a CountVectorizer\n",
        "    vectorizer = CountVectorizer()\n",
        "\n",
        "    # Fit and transform to obtain the bag of words\n",
        "    bow_matrix = vectorizer.fit_transform(messages)\n",
        "\n",
        "    # Get the feature names\n",
        "    f_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Convert bow matrix to an array\n",
        "    bow_array = bow_matrix.toarray()\n",
        "\n",
        "    # Calculate word frequency\n",
        "    word_frequency = bow_array.sum(axis=1)\n",
        "\n",
        "    return word_frequency\n",
        "\n",
        "# TF-IDF Features\n",
        "def tfidf_features_extractor(messages):\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(messages)\n",
        "    tfidf_f_names = tfidf_vectorizer.get_feature_names_out()\n",
        "    tfidf_array = tfidf_matrix.toarray()\n",
        "    return tfidf_f_names, tfidf_array\n",
        "\n",
        "# POS Tagging and Sentiment Scores\n",
        "def pos_tagging_and_sentiment_score(messages):\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    pos_tags_and_sent = []\n",
        "\n",
        "    for message in messages:\n",
        "        # POS tagging\n",
        "        tokens = nltk.word_tokenize(message)\n",
        "        pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "        # Sentiment analysis\n",
        "        sentiments = sia.polarity_scores(message)\n",
        "\n",
        "        pos_tags_and_sent.append({\n",
        "            \"POS Tags\": pos_tags,\n",
        "            \"Sentiment\": sentiments\n",
        "        })\n",
        "\n",
        "    return pos_tags_and_sent\n",
        "\n",
        "# Sample messages and labels (not spam and spam)\n",
        "messages = [\n",
        "    \"Hi Sara, I wanted to confirm our meeting scheduled for tomorrow at 10 AM. Please make sure to bring the project report with you. Looking forward to our discussion. Best regards, James\",\n",
        "    \"Congratulations Sara! You've been selected as the lucky winner of our grand prize. Click the link below to claim your free prize worth $1000! Claim Now: https://www.xyz.com Don't miss this once-in-a-lifetime opportunity. Act now! Regards, Tony\"\n",
        "]\n",
        "\n",
        "labels = [\"not spam\", \"spam\"]\n",
        "\n",
        "# Create a DataFrame\n",
        "features_df = pd.DataFrame()\n",
        "\n",
        "# Adding target labels\n",
        "features_df['Label'] = labels\n",
        "\n",
        "# Clean the messages\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "cleaned_msg = []\n",
        "for message in messages:\n",
        "    cleaned_text = cleaning_text(message)\n",
        "    cleaned_words = [word for word in cleaned_text.split() if word not in stop_words]\n",
        "    cleaned_msg.append(\" \".join(cleaned_words))\n",
        "\n",
        "# 1. Word Frequency\n",
        "word_frequency = word_freq_cal(cleaned_msg)\n",
        "features_df['Word Frequency'] = word_frequency\n",
        "\n",
        "# 2. Existence of hyperlinks(lexical feature extraction)\n",
        "features_df['Has Hyperlink'] = [having_hyperlink(message) for message in cleaned_msg]\n",
        "\n",
        "# 3. TF-IDF Features\n",
        "tfidf_f_names, tfidf_array = tfidf_features_extractor(cleaned_msg)\n",
        "for i, feature_name in enumerate(tfidf_f_names):\n",
        "    features_df[feature_name] = tfidf_array[:, i]\n",
        "\n",
        "# 4. POS Tagging\n",
        "pos_tags_and_sentiments = pos_tagging_and_sentiment_score(cleaned_msg)\n",
        "features_df['POS Tags'] = [str(tags['POS Tags']) for tags in pos_tags_and_sentiments]\n",
        "\n",
        "# 5. Sentiment Scores\n",
        "sentiments = [sentiment['Sentiment'] for sentiment in pos_tags_and_sentiments]\n",
        "features_df['Sentiment Score'] = sentiments\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "# Print features\n",
        "print(\"Features:\")\n",
        "print(features_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcK6Ur8balsV",
        "outputId": "600300b4-3ecd-4941-fc08-e0c832d3d3e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features:\n",
            "      Label  Word Frequency  Has Hyperlink       10     1000      act  \\\n",
            "0  not spam              20              0  0.22934  0.00000  0.00000   \n",
            "1      spam              25              0  0.00000  0.18894  0.18894   \n",
            "\n",
            "      best    bring    claim    click  ...     sure  tomorrow     tony  \\\n",
            "0  0.22934  0.22934  0.00000  0.00000  ...  0.22934   0.22934  0.00000   \n",
            "1  0.00000  0.00000  0.37788  0.18894  ...  0.00000   0.00000  0.18894   \n",
            "\n",
            "    wanted   winner    worth      www      xyz  \\\n",
            "0  0.22934  0.00000  0.00000  0.00000  0.00000   \n",
            "1  0.00000  0.18894  0.18894  0.18894  0.18894   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                    POS Tags  \\\n",
            "0                                                                                [('hi', 'NN'), ('sara', 'NN'), ('wanted', 'VBD'), ('confirm', 'JJ'), ('meeting', 'NN'), ('scheduled', 'VBN'), ('tomorrow', 'NN'), ('10', 'CD'), ('please', 'NN'), ('make', 'VB'), ('sure', 'JJ'), ('bring', 'NN'), ('project', 'NN'), ('report', 'NN'), ('looking', 'VBG'), ('forward', 'JJ'), ('discussion', 'NN'), ('best', 'JJS'), ('regards', 'NNS'), ('james', 'NNS')]   \n",
            "1  [('congratulations', 'NNS'), ('sara', 'VBP'), ('selected', 'VBN'), ('lucky', 'JJ'), ('winner', 'NN'), ('grand', 'JJ'), ('prize', 'NN'), ('click', 'NN'), ('link', 'NN'), ('claim', 'NN'), ('free', 'JJ'), ('prize', 'NN'), ('worth', 'JJ'), ('1000', 'CD'), ('claim', 'NN'), ('https', 'NN'), ('www', 'NN'), ('xyz', 'NNP'), ('com', 'NN'), ('miss', 'NN'), ('lifetime', 'NN'), ('opportunity', 'NN'), ('act', 'NN'), ('regards', 'NNS'), ('tony', 'NN')]   \n",
            "\n",
            "                                                  Sentiment Score  \n",
            "0    {'neg': 0.0, 'neu': 0.659, 'pos': 0.341, 'compound': 0.8316}  \n",
            "1  {'neg': 0.036, 'neu': 0.336, 'pos': 0.629, 'compound': 0.9788}  \n",
            "\n",
            "[2 rows x 46 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The feature selection method I used in the below code is the filter based model for selecting features.\n",
        "#It uses the chi-squared score for selecting the top features.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "\n",
        "# Sample messages\n",
        "data = {\n",
        "    'text': [\n",
        "        \"Hi Sara,I wanted to confirm our meeting scheduled for tomorrow at 10 AM. Please make sure to bring the project report with you.Looking forward to our discussion.Best regards,James\",\n",
        "        \"Congratulations Sara! You've been selected as the lucky winner of our grand prize. Click the link below to claim your free prize worth $1000! Claim Now: https://www.xyz.com Don't miss this once-in-a-lifetime opportunity. Act now! Regards,Tony\"\n",
        "    ],\n",
        "    'label': [0, 1]  # 0 for not spam (ham), 1 for spam\n",
        "}\n",
        "\n",
        "# function to clean text data\n",
        "def clean_text(text):\n",
        "    # Remove special characters, punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Clean the text data\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Split data\n",
        "X = df['cleaned_text']\n",
        "y = df['label']\n",
        "\n",
        "# Vectorize text data\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "# Apply feature selection (chi-squared test as an example)\n",
        "num_top_features = 10  # Updated number of top features to select\n",
        "selector = SelectKBest(score_func=chi2, k=num_top_features)\n",
        "X_new = selector.fit_transform(X_tfidf, y)\n",
        "\n",
        "# Get the chi-squared scores for all features\n",
        "chi2_scores = selector.scores_\n",
        "\n",
        "# Create a DataFrame to store feature names and their chi-squared scores\n",
        "feature_scores_df = pd.DataFrame({'Feature': tfidf_vectorizer.get_feature_names_out(),\n",
        "                                   'Chi2_Score': chi2_scores})\n",
        "\n",
        "# Sort the features by chi-squared score in descending order to rank them\n",
        "ranked_features_df = feature_scores_df.sort_values(by='Chi2_Score', ascending=False)\n",
        "\n",
        "print(\"top 10 selected features\")\n",
        "print(ranked_features_df['Feature'][:10].to_list())\n",
        "\n",
        "# Print the ranked features\n",
        "print(\"Ranked top 10 Features\")\n",
        "print(ranked_features_df.head(10))\n",
        "\n",
        "# Print the ranked features\n",
        "print(\"Ranked Features (Descending Order):\")\n",
        "print(ranked_features_df)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x52bbu2Jgqru",
        "outputId": "668738c7-fa62-4f60-e4e7-38b2ea99a8cd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top 10 selected features\n",
            "['now', 'claim', 'prize', 'meeting', 'please', 'project', 'regardsjames', 'report', 'sarai', 'hi']\n",
            "Ranked top 10 Features\n",
            "         Feature  Chi2_Score\n",
            "26           now    0.316080\n",
            "9          claim    0.316080\n",
            "32         prize    0.316080\n",
            "24       meeting    0.185416\n",
            "31        please    0.185416\n",
            "33       project    0.185416\n",
            "34  regardsjames    0.185416\n",
            "36        report    0.185416\n",
            "38         sarai    0.185416\n",
            "19            hi    0.185416\n",
            "Ranked Features (Descending Order):\n",
            "            Feature  Chi2_Score\n",
            "26              now    0.316080\n",
            "9             claim    0.316080\n",
            "32            prize    0.316080\n",
            "24          meeting    0.185416\n",
            "31           please    0.185416\n",
            "33          project    0.185416\n",
            "34     regardsjames    0.185416\n",
            "36           report    0.185416\n",
            "38            sarai    0.185416\n",
            "19               hi    0.185416\n",
            "39        scheduled    0.185416\n",
            "41             sure    0.185416\n",
            "45         tomorrow    0.185416\n",
            "46           wanted    0.185416\n",
            "48             with    0.185416\n",
            "50       youlooking    0.185416\n",
            "23             make    0.185416\n",
            "0                10    0.185416\n",
            "8             bring    0.185416\n",
            "11          confirm    0.185416\n",
            "13   discussionbest    0.185416\n",
            "16          forward    0.185416\n",
            "15              for    0.185416\n",
            "3                am    0.185416\n",
            "5                at    0.185416\n",
            "43             this    0.158040\n",
            "7             below    0.158040\n",
            "40         selected    0.158040\n",
            "6              been    0.158040\n",
            "18            grand    0.158040\n",
            "37             sara    0.158040\n",
            "47           winner    0.158040\n",
            "49            worth    0.158040\n",
            "2               act    0.158040\n",
            "51             your    0.158040\n",
            "4                as    0.158040\n",
            "35      regardstony    0.158040\n",
            "10            click    0.158040\n",
            "17             free    0.158040\n",
            "12  congratulations    0.158040\n",
            "14             dont    0.158040\n",
            "29      opportunity    0.158040\n",
            "28  onceinalifetime    0.158040\n",
            "27               of    0.158040\n",
            "1              1000    0.158040\n",
            "25             miss    0.158040\n",
            "22            lucky    0.158040\n",
            "21             link    0.158040\n",
            "20   httpswwwxyzcom    0.158040\n",
            "52            youve    0.158040\n",
            "44               to    0.157953\n",
            "30              our    0.060917\n",
            "42              the    0.024223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1342639c-7fc4-42df-b55a-6173fb85b161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Keyword arguments {'many': True} not recognized.\n",
            "Keyword arguments {'many': True} not recognized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1: Similarity Score = 0.7355\n",
            "Hi Sara, I wanted to confirm our meeting scheduled for tomorrow at 10 AM. Please make sure to bring the project report with you. Looking forward to our discussion. Best regards, James\n",
            "==================================================\n",
            "Document 2: Similarity Score = 0.6099\n",
            "Congratulations Sara! You've been selected as the lucky winner of our grand prize. Click the link below to claim your free prize worth $1000! Claim Now: https://www.xyz.com Don't miss this once-in-a-lifetime opportunity. Act now! Regards, Tony\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#query\n",
        "query = \"Please confirm our scheduled meeting for tomorrow at 10 AM.\"\n",
        "\n",
        "#documents\n",
        "docs = [\n",
        "    \"Hi Sara, I wanted to confirm our meeting scheduled for tomorrow at 10 AM. Please make sure to bring the project report with you. Looking forward to our discussion. Best regards, James\",\n",
        "    \"Congratulations Sara! You've been selected as the lucky winner of our grand prize. Click the link below to claim your free prize worth $1000! Claim Now: https://www.xyz.com Don't miss this once-in-a-lifetime opportunity. Act now! Regards, Tony\"\n",
        "]\n",
        "\n",
        "# Load the BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Encoding the query and docs\n",
        "encoded_query = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "encoded_docs = tokenizer(docs, padding=True, truncation=True, return_tensors=\"pt\", many=True)\n",
        "\n",
        "# Getting embeddings by BERT model\n",
        "with torch.no_grad():\n",
        "    query_embedding = model(**encoded_query).last_hidden_state.mean(dim=1)\n",
        "    document_embeddings = model(**encoded_docs).last_hidden_state.mean(dim=1)\n",
        "\n",
        "# Calculate cosine similarity\n",
        "cosine_sim = cosine_similarity(query_embedding, document_embeddings)\n",
        "\n",
        "# Rank docs\n",
        "document_ranking = sorted(enumerate(cosine_sim[0]), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the docs\n",
        "for idx, score in document_ranking:\n",
        "    print(f\"Document {idx + 1}: Similarity Score = {score:.4f}\")\n",
        "    print(docs[idx])\n",
        "    print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJd-lyNEaTD9",
        "outputId": "0aba6979-e6fb-4ab0-d238-383d431a148a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}